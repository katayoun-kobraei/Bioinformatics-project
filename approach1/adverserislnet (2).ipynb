{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nfrom torchtext.transforms import CLIPTokenizer\nfrom torchtext.transforms import PadTransform\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom ast import literal_eval\nfrom sklearn import preprocessing\nimport sqlite3\nimport torch.nn as nn\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-13T08:34:54.758790Z","iopub.execute_input":"2023-07-13T08:34:54.759319Z","iopub.status.idle":"2023-07-13T08:35:01.199472Z","shell.execute_reply.started":"2023-07-13T08:34:54.759283Z","shell.execute_reply":"2023-07-13T08:35:01.198488Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"df_drug = pd.read_pickle('/kaggle/input/datasetname/df.pkl')\nconn = sqlite3.connect(\"/kaggle/input/datasetname/event.db\")\nextraction = pd.read_sql('select * from extraction;', conn)\nextraction.drop(columns=['index'], inplace = True)\ndf_drug.drop(columns=['id', 'index'], inplace = True)\n\ndef feature_extractor(df, f_list):\n    for feature in f_list:\n        unique = set('|'.join(df[feature].values.tolist()).split('|'))\n\n        for side in unique:\n            df[side] = 0\n\n        for index, row in df.iterrows():\n            for side in row[feature].split('|'):\n                df.at[index, side] = 1\n    df.drop(columns=f_list, inplace=True)\n\nfeature_extractor(df_drug, ['side', 'target', 'enzyme', 'pathway', 'smile'])\n\nextraction['side'] = extraction['mechanism'] + extraction['action']\nextraction.drop(columns=['mechanism', 'action'], inplace=True)\nle = preprocessing.LabelEncoder()\nextraction['side'] = le.fit_transform(extraction['side'])","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:35:01.201309Z","iopub.execute_input":"2023-07-13T08:35:01.203297Z","iopub.status.idle":"2023-07-13T08:35:27.727600Z","shell.execute_reply.started":"2023-07-13T08:35:01.203257Z","shell.execute_reply":"2023-07-13T08:35:27.726587Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class DDIDataset(Dataset):\n    def __init__(self, df, extraction):\n        self.extraction = extraction\n        self.df = df\n\n    def __len__(self):\n        return len(self.extraction)\n\n    def __getitem__(self, idx):\n        drugA = torch.tensor(self.df[self.df['name'] == self.extraction.loc[idx]['drugA']].drop(columns=['name']).values.astype('float32'))\n        drugB = torch.tensor(self.df[self.df['name'] == self.extraction.loc[idx]['drugB']].drop(columns=['name']).values.astype('float32'))\n        return torch.cat([(drugA), (drugB)]).flatten(), self.extraction.loc[idx]['side']","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:35:27.729128Z","iopub.execute_input":"2023-07-13T08:35:27.729493Z","iopub.status.idle":"2023-07-13T08:35:27.737716Z","shell.execute_reply.started":"2023-07-13T08:35:27.729456Z","shell.execute_reply":"2023-07-13T08:35:27.736310Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class AdversarialAutoencoder(nn.Module):\n    def __init__(self):\n        super(AdversarialAutoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(25658, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(4096, 2048),\n            nn.BatchNorm1d(2048),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(2048, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(4096, 25658),\n            nn.Sigmoid()\n        )\n        self.discriminator = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(2048, 512 + 256),\n            nn.BatchNorm1d(512 + 256),\n            nn.Dropout(0.2),\n            nn.ReLU(),\n            nn.Linear(512 + 256, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, 65),\n            nn.LogSoftmax()\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        classification = self.classifier(encoded)\n\n        random_latent = torch.randn_like(encoded)\n\n        real_output = self.discriminator(encoded)\n        fake_output = self.discriminator(random_latent)\n\n        return decoded, classification, real_output, fake_output","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:35:27.740696Z","iopub.execute_input":"2023-07-13T08:35:27.741117Z","iopub.status.idle":"2023-07-13T08:35:27.755111Z","shell.execute_reply.started":"2023-07-13T08:35:27.741083Z","shell.execute_reply":"2023-07-13T08:35:27.754086Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def train_adversarial_autoencoder(model, train_loader, test_loader, decoder_criterion, classifier_criterion, adversarial_criterion, optimizer, num_epochs):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model = model.to(device)\n    for epoch in range(num_epochs):\n        model.train() # Set model to train mode\n        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (training)\", leave=False)\n        running_loss = 0.0\n        running_decoder_loss = 0.0\n        running_classifier_loss = 0.0\n        running_correct = 0\n        running_total = 0\n        for i, (inputs, labels) in enumerate(train_pbar):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            reconstructed_inputs, predicted_labels, real_output, fake_output = model(inputs)\n            decoder_loss = decoder_criterion(reconstructed_inputs, inputs)\n            classifier_loss = classifier_criterion(predicted_labels, labels)\n            \n            real_labels = torch.ones_like(real_output)\n            fake_labels = torch.zeros_like(fake_output)\n            adversarial_loss = adversarial_criterion(real_output, real_labels) + adversarial_criterion(fake_output, fake_labels)\n            \n            loss = 0.5 * decoder_loss + classifier_loss - adversarial_loss\n            \n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            running_decoder_loss += decoder_loss.item()\n            running_classifier_loss += classifier_loss.item()\n            _, predicted = torch.max(predicted_labels.data, 1)\n            running_total += labels.size(0)\n            running_correct += (predicted == labels).sum().item()\n            train_pbar.set_postfix({\"loss\": running_loss / (i+1),\n                                    \"decoder_loss\": running_decoder_loss / (i+1),\n                                    \"classifier_loss\": running_classifier_loss / (i+1),\n                                    \"accuracy\": 100 * running_correct / running_total})\n            \n            optimizer.zero_grad()\n            random_latent = torch.randn_like(real_output)\n            real_loss = adversarial_criterion(real_output, real_labels)\n            fake_loss = adversarial_criterion(fake_output, fake_labels)\n            discriminator_loss = real_loss + fake_loss\n            \n            optimizer.step()\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_decoder_loss = running_decoder_loss / len(train_loader)\n        epoch_classifier_loss = running_classifier_loss / len(train_loader)\n        epoch_train_accuracy = 100 * running_correct / running_total\n        train_pbar.set_postfix({\"loss\": epoch_loss,\n                                \"decoder_loss\": epoch_decoder_loss,\n                                \"classifier_loss\": epoch_classifier_loss,\n                                \"accuracy\": epoch_train_accuracy})\n        \n        model.eval() \n        test_running_loss = 0.0\n        test_running_correct = 0\n        test_running_total = 0\n        test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (testing)\", leave=False)\n        with torch.no_grad():\n            for inputs, labels in test_pbar:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                reconstructed_inputs, predicted_labels, _, _ = model(inputs)\n                decoder_loss = decoder_criterion(reconstructed_inputs, inputs)\n                classifier_loss = classifier_criterion(predicted_labels, labels)\n                loss = decoder_loss + classifier_loss\n                test_running_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(predicted_labels.data, 1)\n                test_running_total += labels.size(0)\n                test_running_correct += (predicted == labels).sum().item()\n            test_accuracy = 100 * test_running_correct / test_running_total\n            test_loss = test_running_loss / len(test_loader.dataset)\n        test_pbar.set_postfix({\"loss\": test_loss,\n                               \"accuracy\": test_accuracy})\n        \n        print('Epoch [{}/{}], Train Loss: {:.4f}, Train Autoencoder Loss: {:.4f}, Train Classification Loss: {:.4f}, Train Accuracy: {:.2f}%, Test Loss: {:.4f}, Test Accuracy: {:.2f}%'.format(epoch+1, num_epochs, epoch_loss, epoch_decoder_loss, epoch_classifier_loss, epoch_train_accuracy, test_loss, test_accuracy))","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:35:27.756633Z","iopub.execute_input":"2023-07-13T08:35:27.757129Z","iopub.status.idle":"2023-07-13T08:35:27.777416Z","shell.execute_reply.started":"2023-07-13T08:35:27.757095Z","shell.execute_reply":"2023-07-13T08:35:27.776354Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, random_split\nfrom sklearn.model_selection import KFold\n\nclass AdversarialAutoencoderTrainer:\n    def __init__(self, df_drug, extraction, num_folds=5, batch_size=64, lr=0.001, num_epochs=20):\n        self.df_drug = df_drug\n        self.extraction = extraction\n        self.num_folds = num_folds\n        self.batch_size = batch_size\n        self.lr = lr\n        self.num_epochs = num_epochs\n        \n    def train(self):\n        dataset = DDIDataset(self.df_drug, self.extraction)\n        kf = KFold(n_splits=self.num_folds, shuffle=True, random_state=42)\n        fold = 1\n        for train_index, test_index in kf.split(dataset):\n            train_dataset = torch.utils.data.Subset(dataset, train_index)\n            test_dataset = torch.utils.data.Subset(dataset, test_index)\n            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n            test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n            model = AdversarialAutoencoder()\n            decoder_criterion = nn.MSELoss() \n            adversarial_criterion = nn.MSELoss()\n            classifier_criterion = nn.CrossEntropyLoss() \n            optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n            print(\"Training fold {}...\".format(fold))\n            train_adversarial_autoencoder(model, train_loader, test_loader, decoder_criterion, classifier_criterion, adversarial_criterion, optimizer, self.num_epochs)\n            fold += 1","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:35:39.954427Z","iopub.execute_input":"2023-07-13T08:35:39.954858Z","iopub.status.idle":"2023-07-13T08:35:39.968488Z","shell.execute_reply.started":"2023-07-13T08:35:39.954822Z","shell.execute_reply":"2023-07-13T08:35:39.967463Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"trainer = AdversarialAutoencoderTrainer(df_drug, extraction)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:35:41.638160Z","iopub.execute_input":"2023-07-13T08:35:41.638600Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training fold 1...\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20], Train Loss: -0.3980, Train Autoencoder Loss: 0.0208, Train Classification Loss: 1.3480, Train Accuracy: 63.61%, Test Loss: 0.9629, Test Accuracy: 71.90%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [2/20], Train Loss: -1.1230, Train Autoencoder Loss: 0.0165, Train Classification Loss: 0.7941, Train Accuracy: 75.32%, Test Loss: 0.7517, Test Accuracy: 77.02%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [3/20], Train Loss: -1.3046, Train Autoencoder Loss: 0.0147, Train Classification Loss: 0.6204, Train Accuracy: 80.08%, Test Loss: 0.6443, Test Accuracy: 79.94%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [4/20], Train Loss: -1.4370, Train Autoencoder Loss: 0.0133, Train Classification Loss: 0.5087, Train Accuracy: 83.04%, Test Loss: 0.5971, Test Accuracy: 81.09%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [5/20], Train Loss: -1.5254, Train Autoencoder Loss: 0.0122, Train Classification Loss: 0.4249, Train Accuracy: 85.57%, Test Loss: 0.5756, Test Accuracy: 82.11%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [6/20], Train Loss: -1.5915, Train Autoencoder Loss: 0.0113, Train Classification Loss: 0.3624, Train Accuracy: 87.66%, Test Loss: 0.5547, Test Accuracy: 82.80%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [7/20], Train Loss: -1.6413, Train Autoencoder Loss: 0.0106, Train Classification Loss: 0.3141, Train Accuracy: 89.36%, Test Loss: 0.5346, Test Accuracy: 83.62%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch [8/20], Train Loss: -1.6759, Train Autoencoder Loss: 0.0099, Train Classification Loss: 0.2833, Train Accuracy: 90.33%, Test Loss: 0.5224, Test Accuracy: 84.60%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch [9/20], Train Loss: -1.7090, Train Autoencoder Loss: 0.0094, Train Classification Loss: 0.2503, Train Accuracy: 91.39%, Test Loss: 0.5240, Test Accuracy: 84.57%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch [10/20], Train Loss: -1.7408, Train Autoencoder Loss: 0.0088, Train Classification Loss: 0.2217, Train Accuracy: 92.22%, Test Loss: 0.5018, Test Accuracy: 85.87%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch [11/20], Train Loss: -1.7650, Train Autoencoder Loss: 0.0084, Train Classification Loss: 0.1953, Train Accuracy: 93.25%, Test Loss: 0.5386, Test Accuracy: 84.60%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch [12/20], Train Loss: -1.7792, Train Autoencoder Loss: 0.0081, Train Classification Loss: 0.1826, Train Accuracy: 93.80%, Test Loss: 0.5247, Test Accuracy: 85.98%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch [13/20], Train Loss: -1.8009, Train Autoencoder Loss: 0.0077, Train Classification Loss: 0.1640, Train Accuracy: 94.44%, Test Loss: 0.5310, Test Accuracy: 85.99%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch [14/20], Train Loss: -1.8147, Train Autoencoder Loss: 0.0074, Train Classification Loss: 0.1542, Train Accuracy: 94.81%, Test Loss: 0.5147, Test Accuracy: 86.62%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch [15/20], Train Loss: -1.8332, Train Autoencoder Loss: 0.0070, Train Classification Loss: 0.1361, Train Accuracy: 95.39%, Test Loss: 0.5386, Test Accuracy: 86.68%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch [16/20], Train Loss: -1.8348, Train Autoencoder Loss: 0.0069, Train Classification Loss: 0.1313, Train Accuracy: 95.59%, Test Loss: 0.5481, Test Accuracy: 86.82%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch [17/20], Train Loss: -1.8494, Train Autoencoder Loss: 0.0066, Train Classification Loss: 0.1209, Train Accuracy: 95.96%, Test Loss: 0.5310, Test Accuracy: 86.78%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch [18/20], Train Loss: -1.8629, Train Autoencoder Loss: 0.0064, Train Classification Loss: 0.1076, Train Accuracy: 96.50%, Test Loss: 0.5440, Test Accuracy: 87.29%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch [19/20], Train Loss: -1.8564, Train Autoencoder Loss: 0.0062, Train Classification Loss: 0.1152, Train Accuracy: 96.23%, Test Loss: 0.5691, Test Accuracy: 86.82%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch [20/20], Train Loss: -1.8779, Train Autoencoder Loss: 0.0059, Train Classification Loss: 0.0944, Train Accuracy: 96.85%, Test Loss: 0.5471, Test Accuracy: 87.15%\nTraining fold 2...\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20], Train Loss: -0.2067, Train Autoencoder Loss: 0.0214, Train Classification Loss: 1.3420, Train Accuracy: 64.14%, Test Loss: 0.9528, Test Accuracy: 72.62%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [2/20], Train Loss: -1.1182, Train Autoencoder Loss: 0.0170, Train Classification Loss: 0.8115, Train Accuracy: 74.95%, Test Loss: 0.7546, Test Accuracy: 77.31%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch [3/20], Train Loss: -1.3205, Train Autoencoder Loss: 0.0152, Train Classification Loss: 0.6219, Train Accuracy: 80.13%, Test Loss: 0.6750, Test Accuracy: 79.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20 (training):  33%|███▎      | 156/466 [01:01<01:58,  2.61it/s, loss=-1.45, decoder_loss=0.014, classifier_loss=0.496, accuracy=83.4] ","output_type":"stream"}]},{"cell_type":"code","source":"# from torch.utils.data import random_split\n\n# dataset = DDIDataset(df_drug, extraction)\n# train_size = int(0.8 * len(dataset))\n# test_size = len(dataset) - train_size\n# train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# model = AdversarialAutoencoder()\n# decoder_criterion = nn.MSELoss() \n# adversarial_criterion = nn.MSELoss()\n# classifier_criterion = nn.CrossEntropyLoss() \n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n# num_epochs = 100\n\n# train_adversarial_autoencoder(model, train_loader, test_loader, decoder_criterion, classifier_criterion, adversarial_criterion, optimizer, num_epochs)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:35:38.037203Z","iopub.status.idle":"2023-07-13T08:35:38.040612Z","shell.execute_reply.started":"2023-07-13T08:35:38.040349Z","shell.execute_reply":"2023-07-13T08:35:38.040374Z"},"trusted":true},"execution_count":null,"outputs":[]}]}